# -*- coding: utf-8 -*-
import os
import sys
import hashlib
from pathlib import Path
import streamlit as st

# =========================================================
# 1. sqlite3 í˜¸í™˜ (Chroma ì•ˆì •í™”)
# =========================================================
try:
    __import__("pysqlite3")
    sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")
except Exception:
    pass

# =========================================================
# 2. LangChain / Chroma
# =========================================================
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_chroma import Chroma

# =========================================================
# 3. OpenAI API Key
# =========================================================
if "OPENAI_API_KEY" in st.secrets:
    os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# =========================================================
# 4. Streamlit ê¸°ë³¸ ì„¤ì •
# =========================================================
st.set_page_config(page_title="PDF RAG ì±—ë´‡", page_icon="ğŸ“š")
st.header("ğŸ“š PDF ê¸°ë°˜ RAG ì±—ë´‡")

# =========================================================
# 5. PDF ì—…ë¡œë“œ
# =========================================================
uploaded = st.file_uploader("ğŸ“„ PDF íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])

if not uploaded:
    st.info("PDFë¥¼ ì—…ë¡œë“œí•˜ë©´ ì§ˆë¬¸ ì…ë ¥ì°½ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")
    st.stop()

# =========================================================
# 6. PDF ì €ì¥ + ê³ ìœ  ID
# =========================================================
tmp_dir = Path(".streamlit_tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)

pdf_bytes = uploaded.getvalue()
file_id = hashlib.sha256(pdf_bytes).hexdigest()[:12]

pdf_path = tmp_dir / uploaded.name
pdf_path.write_bytes(pdf_bytes)

persist_dir = f"./chroma_db/{file_id}"

# =========================================================
# 7. PDF ë¡œë“œ
#    âŒ ìºì‹œ ì‚¬ìš© ì•ˆ í•¨ (Document ê°ì²´ ë•Œë¬¸)
# =========================================================
pages = PyPDFLoader(str(pdf_path)).load()

# =========================================================
# 8. VectorStore ìƒì„± ë˜ëŠ” ë¡œë“œ
#    âŒ ìºì‹œ ì‚¬ìš© ì•ˆ í•¨ (í•µì‹¬)
# =========================================================
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

if os.path.isdir(persist_dir) and any(os.scandir(persist_dir)):
    vectorstore = Chroma(
        persist_directory=persist_dir,
        embedding_function=embeddings
    )
else:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=150
    )
    split_docs = splitter.split_documents(pages)

    vectorstore = Chroma.from_documents(
        split_docs,
        embeddings,
        persist_directory=persist_dir
    )

retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# =========================================================
# 9. RAG Chain
# =========================================================
contextualize_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "ì´ì „ ëŒ€í™”ë¥¼ ì°¸ê³ í•´ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¿”ë¼."),
        MessagesPlaceholder("history"),
        ("human", "{input}")
    ]
)

qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system",
         "ë„ˆëŠ” ë°˜ë“œì‹œ PDFì—ì„œ ê²€ìƒ‰ëœ ë‚´ìš©(context)ë§Œìœ¼ë¡œ ë‹µí•´ì•¼ í•œë‹¤.\n"
         "contextì— ê·¼ê±°ê°€ ì—†ìœ¼ë©´ ë°˜ë“œì‹œ 'PDFì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•˜ë¼.\n\n"
         "{context}"
        ),
        MessagesPlaceholder("history"),
        ("human", "{input}")
    ]
)

llm = ChatOpenAI(model="gpt-4o-mini")

history_aware_retriever = create_history_aware_retriever(
    llm, retriever, contextualize_prompt
)

qa_chain = create_stuff_documents_chain(llm, qa_prompt)

rag_chain = create_retrieval_chain(
    history_aware_retriever,
    qa_chain
)

chat_history = StreamlitChatMessageHistory(key="chat_messages")

conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer",
)

# =========================================================
# 10. ì±„íŒ… UI (ì§ˆë¬¸ ì…ë ¥ì°½ ì •ìƒ)
# =========================================================
for msg in chat_history.messages:
    st.chat_message(msg.type).write(msg.content)

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    st.chat_message("human").write(prompt)

    with st.chat_message("ai"):
        with st.spinner("Thinking..."):
            config = {"configurable": {"session_id": "any"}}
            response = conversational_rag_chain.invoke(
                {"input": prompt},
                config
            )

            st.write(response.get("answer", ""))

            # ğŸ” ë””ë²„ê·¸ íŒ¨ë„
            with st.expander("ğŸ” RAG ë””ë²„ê·¸"):
                ctx = response.get("context", [])
                st.write("ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜:", len(ctx))
                st.write("PDF ê²½ë¡œ:", pdf_path)
                st.write("DB ê²½ë¡œ:", persist_dir)

                for i, doc in enumerate(ctx, 1):
                    st.markdown(f"### ë¬¸ì„œ {i}")
                    st.code(doc.page_content[:400])
