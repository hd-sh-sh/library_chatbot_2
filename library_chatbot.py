# -*- coding: utf-8 -*-
import os
import sys
import streamlit as st
from pathlib import Path

# =========================================================
# sqlite3 í˜¸í™˜ (Chroma ì˜¤ë¥˜ ë°©ì§€)
# =========================================================
try:
    __import__("pysqlite3")
    sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")
except Exception:
    pass

# =========================================================
# LangChain imports
# =========================================================
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_chroma import Chroma

# =========================================================
# OpenAI API KEY
# =========================================================
if "OPENAI_API_KEY" in st.secrets:
    os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# =========================================================
# PDF ë¡œë“œ í•¨ìˆ˜ (ìºì‹œ OK)
# =========================================================
@st.cache_resource(show_spinner=False)
def load_and_split_pdf(file_path: str):
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()

# =========================================================
# VectorStore ìƒì„±/ë¡œë“œ (ìºì‹œ ì‚¬ìš© âŒ)
# =========================================================
def build_or_load_vectorstore(docs, persist_directory="./chroma_db"):
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    if os.path.isdir(persist_directory) and any(os.scandir(persist_directory)):
        return Chroma(
            persist_directory=persist_directory,
            embedding_function=embeddings
        )

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=150
    )
    split_docs = splitter.split_documents(docs)

    return Chroma.from_documents(
        split_docs,
        embeddings,
        persist_directory=persist_directory
    )

# =========================================================
# RAG Chain ì´ˆê¸°í™”
# =========================================================
def initialize_chain(selected_model: str, pdf_path: str):
    pages = load_and_split_pdf(pdf_path)
    vectorstore = build_or_load_vectorstore(pages)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

    # ì§ˆë¬¸ ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", "ì´ì „ ëŒ€í™”ë¥¼ ì°¸ê³ í•´ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¿”ë¼."),
            MessagesPlaceholder("history"),
            ("human", "{input}")
        ]
    )

    # âœ… PDFì— ì—†ëŠ” ë‚´ìš©ì€ ë°˜ë“œì‹œ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ê²Œ ê°•ì œ
    qa_system_prompt = (
        "ë„ˆëŠ” PDF ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ë„ìš°ë¯¸ì´ë‹¤.\n"
        "ë°˜ë“œì‹œ ì•„ë˜ ë¬¸ì„œ ë‚´ìš©(context)ì— ê·¼ê±°í•´ì„œë§Œ ë‹µë³€í•´ì•¼ í•œë‹¤.\n"
        "ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì´ê±°ë‚˜ ê·¼ê±°ê°€ ì—†ìœ¼ë©´\n"
        "ë°˜ë“œì‹œ 'í•´ë‹¹ ë‚´ìš©ì€ ì œê³µëœ PDF ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•˜ë¼.\n"
        "ì ˆëŒ€ ì¶”ì¸¡í•˜ê±°ë‚˜ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µí•˜ì§€ ë§ˆë¼.\n"
        "ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ë¼.\n\n"
        "{context}"
    )

    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}")
        ]
    )

    llm = ChatOpenAI(model=selected_model)

    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

    return create_retrieval_chain(
        history_aware_retriever,
        question_answer_chain
    )

# =========================================================
# Streamlit UI
# =========================================================
st.set_page_config(page_title="êµ­ë¦½ë¶€ê²½ëŒ€ ë„ì„œê´€ ê·œì • Q&A", page_icon="ğŸ“š")
st.header("êµ­ë¦½ë¶€ê²½ëŒ€ ë„ì„œê´€ ê·œì • Q&A ì±—ë´‡ ğŸ’¬ğŸ“š")

# ëª¨ë¸ ì„ íƒ
option = st.selectbox(
    "GPT ëª¨ë¸ ì„ íƒ",
    ("gpt-4o-mini", "gpt-3.5-turbo-0125")
)

# =========================================================
# PDF ì—…ë¡œë“œ
# =========================================================
DEFAULT_PDF = "[ì±—ë´‡í”„ë¡œê·¸ë¨ë°ì‹¤ìŠµ] ë¶€ê²½ëŒ€í•™êµ ê·œì •ì§‘.pdf"
uploaded = st.file_uploader("PDFë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ê¸°ë³¸ PDFë¡œ ì‹¤í–‰í•˜ì„¸ìš”", type=["pdf"])

pdf_path = None

if uploaded is not None:
    tmp_dir = Path(".streamlit_tmp")
    tmp_dir.mkdir(parents=True, exist_ok=True)
    pdf_path = tmp_dir / uploaded.name
    pdf_path.write_bytes(uploaded.getbuffer())
else:
    if os.path.exists(DEFAULT_PDF):
        pdf_path = DEFAULT_PDF

if not pdf_path:
    st.info("ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    st.stop()

# =========================================================
# RAG ì²´ì¸ + ì±„íŒ…
# =========================================================
rag_chain = initialize_chain(option, str(pdf_path))
chat_history = StreamlitChatMessageHistory(key="chat_messages")

conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer"
)

# ê¸°ì¡´ ëŒ€í™” í‘œì‹œ
for msg in chat_history.messages:
    st.chat_message(msg.type).write(msg.content)

# ì§ˆë¬¸ ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    st.chat_message("human").write(prompt)

    with st.chat_message("ai"):
        with st.spinner("Thinking..."):
            response = conversational_rag_chain.invoke(
                {"input": prompt},
                {"configurable": {"session_id": "any"}}
            )

            st.write(response.get("answer", ""))

            with st.expander("ğŸ“„ ì°¸ê³  ë¬¸ì„œ"):
                for doc in response.get("context", []):
                    st.markdown(doc.metadata.get("source", "source"),
                                help=doc.page_content)
